{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wv9q_pcGshE"
      },
      "source": [
        "# Assignment 2 2AMM10 2023-2024\n",
        "\n",
        "## Group: [Fill in your group name]\n",
        "### Member 1: [Fill in your name]\n",
        "### Member 2: [Fill in your name]\n",
        "### Member 3: [Fill in your name]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQzvuDWw_Eyw"
      },
      "source": [
        "We need to install some specific libraries. The cell below installs torch_geometric for torch 2.6.0+cu124. In case the current version of torch is different, check [here](https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html) to see which versions (of both libraries) you should install. You might also need to install an old version of torch from [here](https://pytorch.org/get-started/previous-versions/)\n",
        "\n",
        "**Note:** Do not install pyg_lib from the optional dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ibC2lMHfD67H"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: torch\n",
            "Version: 2.7.0\n",
            "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
            "Home-page: https://pytorch.org/\n",
            "Author: PyTorch Team\n",
            "Author-email: packages@pytorch.org\n",
            "License: BSD-3-Clause\n",
            "Location: c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages\n",
            "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
            "Required-by: pytorch-lightning, torchmetrics\n"
          ]
        }
      ],
      "source": [
        "!pip show torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8qrPQFNe_AJu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rdkit in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (2025.3.2)\n",
            "Requirement already satisfied: numpy in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from rdkit) (2.2.6)\n",
            "Requirement already satisfied: Pillow in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from rdkit) (11.2.1)\n",
            "Requirement already satisfied: torch_geometric in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from torch_geometric) (3.12.6)\n",
            "Requirement already satisfied: fsspec in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from torch_geometric) (2025.5.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from torch_geometric) (2.2.6)\n",
            "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from torch_geometric) (5.9.0)\n",
            "Requirement already satisfied: pyparsing in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from aiohttp->torch_geometric) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from aiohttp->torch_geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from aiohttp->torch_geometric) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from aiohttp->torch_geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from requests->torch_geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from requests->torch_geometric) (2025.4.26)\n",
            "Requirement already satisfied: colorama in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from tqdm->torch_geometric) (0.4.6)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Requirement already satisfied: torch_scatter in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (2.1.2+pt26cu124)\n",
            "Requirement already satisfied: torch_sparse in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (0.6.18+pt26cu124)\n",
            "Requirement already satisfied: torch_cluster in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (1.6.3+pt21cpu)\n",
            "Requirement already satisfied: torch_spline_conv in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (1.2.2+pt26cu124)\n",
            "Requirement already satisfied: scipy in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from torch_sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages (from scipy->torch_sparse) (2.2.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit\n",
        "!pip install torch_geometric\n",
        "!pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.6.0+cu124.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WVL2eo0g_Iuv"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw, AllChem\n",
        "from rdkit import RDLogger\n",
        "RDLogger.DisableLog('rdApp.*')\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from rdkit import Chem\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "H8rvaK56_iQ7"
      },
      "outputs": [],
      "source": [
        "with open('pos_data.pkl', 'rb') as f:\n",
        "    pos_data = pickle.load(f)\n",
        "\n",
        "with open('type_data.pkl', 'rb') as f:\n",
        "    type_data = pickle.load(f)\n",
        "\n",
        "with open('smiles.pkl', 'rb') as f:\n",
        "    smiles_data = pickle.load(f)\n",
        "\n",
        "data_split = np.load('data_split.npz')\n",
        "\n",
        "train_idxes = data_split['train_idx']\n",
        "test_idxes = data_split['test_idx']\n",
        "\n",
        "formation_energy = np.load('formation_energy.npz')\n",
        "\n",
        "fe = formation_energy['y'] # normalized formation energy\n",
        "mu = formation_energy['mu']\n",
        "std = formation_energy['sigma']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DIsGRQcxA_4Q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of data\n",
            "pos_data: 129012, type_data: 129012, smiles: 129012\n",
            "Idxes\n",
            "train: 119012, test: 10000, sum: 129012\n"
          ]
        }
      ],
      "source": [
        "# shapes of lists\n",
        "print(\"Length of data\")\n",
        "print(f\"pos_data: {len(pos_data)}, type_data: {len(type_data)}, smiles: {len(smiles_data)}\")\n",
        "print(\"Idxes\")\n",
        "print(f\"train: {len(train_idxes)}, test: {len(test_idxes)}, sum: {len(train_idxes) + len(test_idxes)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean of formation energy: 0.0004583929\n",
            "Standard deviation of formation energy: 1.0020696\n",
            "Maximum formation energy: 6.492535\n",
            "Minimum formation energy: -3.5815375\n"
          ]
        }
      ],
      "source": [
        "# Check distribution of formation energy\n",
        "plt.hist(fe, bins=50, density=True)\n",
        "plt.xlabel('Normalized Formation Energy')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Distribution of Normalized Formation Energy')\n",
        "plt.show()\n",
        "\n",
        "print(\"Mean of formation energy:\", np.mean(fe))\n",
        "print(\"Standard deviation of formation energy:\", np.std(fe))\n",
        "print(\"Maximum formation energy:\", np.max(fe))\n",
        "print(\"Minimum formation energy:\", np.min(fe))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bVDJF7I3BFa2"
      },
      "outputs": [],
      "source": [
        "def at_number_to_atom_name(at_number):\n",
        "    if at_number == 6:\n",
        "        return 'C'\n",
        "    elif at_number == 1:\n",
        "        return 'H'\n",
        "    elif at_number == 7:\n",
        "        return 'N'\n",
        "    elif at_number == 8:\n",
        "        return 'O'\n",
        "    elif at_number == 9:\n",
        "        return 'F'\n",
        "    elif at_number == 16:\n",
        "        return 'S'\n",
        "    else:\n",
        "        return 'Unknown'\n",
        "\n",
        "def inspect_structure(idx):\n",
        "    smile = smiles_data[idx]\n",
        "    pos = pos_data[idx]\n",
        "    typ = type_data[idx]\n",
        "\n",
        "    header = f\"{'Atom':^5}│{'Number':^6}│{'x':^10}│{'y':^10}│{'z':^10}\"\n",
        "    line   = \"─────┼──────┼──────────┼──────────┼──────────\"\n",
        "    print(header)\n",
        "    print(line)\n",
        "\n",
        "    for atom_num, (x, y, z) in zip(typ, pos):\n",
        "        atom_sym = at_number_to_atom_name(atom_num)\n",
        "        print(f\"{atom_sym:^5}│{atom_num:^6}│{x:>10.3f}│{y:>10.3f}│{z:>10.3f}\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(f'SMILE: {smile}')\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(f'Formation Energy: {fe[idx]*std + mu:.3f}')\n",
        "    print(f'Formation Energy (normalized): {fe[idx]:.5f}')\n",
        "    mol = Chem.MolFromSmiles(smile)\n",
        "    if mol:\n",
        "        # RDKit prefers 2‑D coordinates for nice depictions\n",
        "        Chem.AllChem.Compute2DCoords(mol)\n",
        "        img = Draw.MolToImage(mol, size=(300, 300))\n",
        "\n",
        "        # Display with matplotlib (works both in notebooks and scripts)\n",
        "        plt.figure(figsize=(3, 3))\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K1rs7hhCC4oq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Atom │Number│    x     │    y     │    z     \n",
            "─────┼──────┼──────────┼──────────┼──────────\n",
            "  C  │  6   │    -0.013│     1.086│     0.008\n",
            "  H  │  1   │     0.002│    -0.006│     0.002\n",
            "  H  │  1   │     1.012│     1.464│     0.000\n",
            "  H  │  1   │    -0.541│     1.447│    -0.877\n",
            "  H  │  1   │    -0.524│     1.438│     0.906\n",
            "\n",
            "\n",
            "SMILE: C\n",
            "\n",
            "\n",
            "Formation Energy: -17.172\n",
            "Formation Energy (normalized): 5.72327\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# methane\n",
        "# Note how methane has a relatively high formation energy (compared to QM9)\n",
        "# This correlates with lower thermodynamic stability and higher reactivity\n",
        "# For example, methane readily burns in oxygen (CH₄ + 2O₂ → CO₂ + 2H₂O)\n",
        "inspect_structure(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vo8hYLuQCeBR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Atom │Number│    x     │    y     │    z     \n",
            "─────┼──────┼──────────┼──────────┼──────────\n",
            "  C  │  6   │    -0.143│     1.521│     0.693\n",
            "  C  │  6   │     0.003│     0.009│     0.543\n",
            "  C  │  6   │     0.619│    -0.462│    -0.800\n",
            "  C  │  6   │     1.284│    -1.791│    -0.451\n",
            "  C  │  6   │     2.629│    -1.823│     0.182\n",
            "  C  │  6   │     3.588│    -0.659│     0.233\n",
            "  O  │  8   │     4.471│    -0.752│     1.344\n",
            "  N  │  7   │     1.408│    -1.923│     1.022\n",
            "  C  │  6   │     0.902│    -0.661│     1.609\n",
            "  H  │  1   │     0.830│     2.022│     0.609\n",
            "  H  │  1   │    -0.798│     1.937│    -0.079\n",
            "  H  │  1   │    -0.570│     1.786│     1.666\n",
            "  H  │  1   │    -0.991│    -0.449│     0.625\n",
            "  H  │  1   │    -0.142│    -0.594│    -1.573\n",
            "  H  │  1   │     1.332│     0.276│    -1.184\n",
            "  H  │  1   │     0.937│    -2.679│    -0.974\n",
            "  H  │  1   │     3.162│    -2.773│     0.153\n",
            "  H  │  1   │     4.226│    -0.685│    -0.658\n",
            "  H  │  1   │     3.069│     0.309│     0.222\n",
            "  H  │  1   │     3.936│    -0.919│     2.127\n",
            "  H  │  1   │     1.713│     0.013│     1.915\n",
            "  H  │  1   │     0.338│    -0.908│     2.513\n",
            "\n",
            "\n",
            "SMILE: C[C@H]1C[C@H]2[C@@H](CO)[N@H+]2C1\n",
            "\n",
            "\n",
            "Formation Energy: -88.273\n",
            "Formation Energy (normalized): -1.17243\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# random structure\n",
        "inspect_structure(np.random.choice(range(len(smiles_data))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The available device is cpu\n"
          ]
        }
      ],
      "source": [
        "# Check device settings\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    device = 'cpu'\n",
        "\n",
        "print(f'The available device is {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of SMILES data: 129012\n",
            "First 10 SMILES strings:\n",
            "0: C\n",
            "1: N\n",
            "2: O\n",
            "3: C#C\n",
            "4: C#N\n",
            "5: C=O\n",
            "6: CC\n",
            "7: CO\n",
            "8: C#CC\n",
            "9: CC#N\n",
            "Max length of SMILES strings: 62\n"
          ]
        }
      ],
      "source": [
        "# These are some print statements inserted by me to check the data\n",
        "\n",
        "# Check the length of the SMILES representation\n",
        "print(f'Length of SMILES data: {len(smiles_data)}')\n",
        "# Check the first 10 SMILES strings\n",
        "print(\"First 10 SMILES strings:\")\n",
        "for i in range(10):\n",
        "    print(f\"{i}: {smiles_data[i]}\")\n",
        "# Check max lenght of SMILES strings\n",
        "max_smiles_length = max(len(smile) for smile in smiles_data) # suggestion for max lenght in model\n",
        "print(f'Max length of SMILES strings: {max_smiles_length}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76MeHNQ_Gd9t"
      },
      "source": [
        "## Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PART II: SMILES\n",
        "- String sequence to continous values\n",
        "- Permutational invariance should be taken into account with canonical SMILES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lv736iffCez4"
      },
      "outputs": [],
      "source": [
        "# Tokenize the SMILES strings\n",
        "# https://jcheminf.biomedcentral.com/articles/10.1186/s13321-023-00725-9\n",
        "# Unclear to me whether we want to encode to values, but seems logical\n",
        "class SMILESAISTokenizer:\n",
        "    \"\"\"\n",
        "    For each atom in the molecule, it build a token in the format: \n",
        "    symbol:charge:chirality:hydrogens:ring_status:[neighbors]\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.vocab = set()\n",
        "        self.special_tokens = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
        "        self.token_to_idx = {}\n",
        "        self.idx_to_token = {}\n",
        "    \n",
        "    def build_vocabulary(self, smiles_list):\n",
        "        \"\"\"\n",
        "        Builds a vocabulary from a list of SMILES strings, it collect unique tokens\n",
        "        returns a full vocabulary list\n",
        "        \"\"\"\n",
        "        tokens = set()\n",
        "\n",
        "        for smi in smiles_list:\n",
        "            tokens.update(self.tokenize(smi))\n",
        "\n",
        "        tokens = sorted(list(tokens))\n",
        "        full_vocab = self.special_tokens + tokens\n",
        "        self.vocab = full_vocab\n",
        "        self.token_to_idx = {tok: i for i, tok in enumerate(full_vocab)}\n",
        "        self.idx_to_token = {i: tok for tok, i in self.token_to_idx.items()}\n",
        "\n",
        "        return full_vocab\n",
        "    \n",
        "    def tokenize(self, smiles):\n",
        "        \"\"\" \n",
        "        converts a smiles string into a list of tokens, one per atom\n",
        "        \"\"\"\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            return []\n",
        "\n",
        "        tokens = []\n",
        "        for atom in mol.GetAtoms():\n",
        "            symbol = atom.GetSymbol()\n",
        "            if atom.GetIsAromatic():\n",
        "                symbol = symbol.lower()\n",
        "\n",
        "            charge = atom.GetFormalCharge()\n",
        "            chiral = str(atom.GetChiralTag())\n",
        "            hs = atom.GetTotalNumHs()\n",
        "            ring = 'R' if atom.IsInRing() else '!R'\n",
        "            neighbors = sorted([nbr.GetSymbol() for nbr in atom.GetNeighbors()])\n",
        "\n",
        "            token = f\"{symbol}:{charge}:{chiral}:{hs}:{ring}:[{','.join(neighbors)}]\"\n",
        "            tokens.append(token)\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def encode(self, smiles, max_length=None):\n",
        "        \"\"\"\n",
        "        converts a SMILES string into a list of token indices\n",
        "        \"\"\"\n",
        "        tokens = self.tokenize(smiles)\n",
        "        # Add <SOS> and <EOS> tokens to learn start and end of sequence\n",
        "        tokens = ['<SOS>'] + tokens + ['<EOS>']\n",
        "        #<UNK> if it encounters a token that it has not seen before\n",
        "        encoded = [self.token_to_idx.get(tok, self.token_to_idx['<UNK>']) for tok in tokens]\n",
        "\n",
        "        if max_length:\n",
        "            encoded = encoded[:max_length]\n",
        "            encoded += [self.token_to_idx['<PAD>']] * (max_length - len(encoded))\n",
        "        return encoded\n",
        "\n",
        "    # def decode(self, indices): #is this needed?\n",
        "    #     tokens = [self.idx_to_token.get(idx, '<UNK>') for idx in indices]\n",
        "    #     tokens = [tok for tok in tokens if tok not in self.special_tokens]\n",
        "    #     return tokens # Not sure whether we need decoder\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # TOY TEST\n",
        "# smiles_list = [\n",
        "#     \"CCO\",         # ethanol\n",
        "#     \"c1ccccc1\",    # benzene\n",
        "#     \"C[N+](C)(C)C\",# tetramethylammonium\n",
        "#     \"O=C=O\"        # carbon dioxide\n",
        "# ]\n",
        "\n",
        "# tokenizer = SMILESAISTokenizer()\n",
        "# # Initialize and build vocab\n",
        "# vocab = tokenizer.build_vocabulary(smiles_list)\n",
        "# print(\"Vocabulary size:\", len(vocab))\n",
        "\n",
        "# # Tokenize and encode a molecule\n",
        "# encoded = tokenizer.encode(\"CCO\", max_length=10)\n",
        "# print(\"Encoded:\", encoded)\n",
        "# print(\"Decoded tokens:\", tokenizer.decode(encoded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Randomized augmentation: https://pubs.rsc.org/en/content/articlelanding/2022/dd/d2dd00058j\n",
        "class SMILESDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Takes a list of SMILES strings and formation energies and tokenizes the SMILES strings.\n",
        "    \"\"\"\n",
        "    def __init__(self, tokenizer, smiles_list, formation_energy, augment=True, max_length=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.smiles = smiles_list\n",
        "        self.augment = augment\n",
        "        self.max_length = max_length\n",
        "        self.formation_energy = formation_energy\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.smiles)\n",
        "\n",
        "    def randomize_smiles(self, mol):\n",
        "        \"\"\" \n",
        "        randomizes the SMILES representation of a molecule to a chemically \n",
        "        equivlent SMILES for data augmentation\n",
        "        \"\"\"\n",
        "        return Chem.MolToSmiles(mol, doRandom=True)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        returns single datata sample of a tokenized SMILES and its formation energy\n",
        "        \"\"\"\n",
        "        smiles = self.smiles[idx]\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        # if mol is None: #is this needed?\n",
        "        #     return self.__getitem__((idx + 1) % len(self.smiles))  # fallback\n",
        "\n",
        "        if self.augment:\n",
        "            smiles = self.randomize_smiles(mol)\n",
        "        else:\n",
        "            smiles = Chem.MolToSmiles(mol, canonical=True)\n",
        "\n",
        "        encoded = self.tokenizer.encode(smiles, max_length=self.max_length)\n",
        "        target = self.formation_energy[idx]\n",
        "        return torch.tensor(encoded, dtype=torch.long), torch.tensor(target, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create tokenzier and dataset\n",
        "# Split data into train and validation 80/20, already having a test dataset\n",
        "train_data = [smiles_data[i] for i in train_idxes[:-int(0.2 * len(train_idxes))]] # select first 80% of train_idxes; no overlap measured\n",
        "val_data = [smiles_data[i] for i in train_idxes[-int(0.2 * len(train_idxes)):]] # select last 20% of train_idxes; no overlap measured\n",
        "test_data = [smiles_data[i] for i in test_idxes]\n",
        "\n",
        "tokenizer = SMILESAISTokenizer()\n",
        "tokenizer.build_vocabulary(train_data)\n",
        "max_length = 100 # the  maximum length of SMILES strings is 62, tokenization might change that\n",
        "\n",
        "train_targets = fe[train_idxes[:-int(0.2 * len(train_idxes))]]\n",
        "val_targets = fe[train_idxes[-int(0.2 * len(train_idxes)):]]\n",
        "test_targets = fe[test_idxes]\n",
        "\n",
        "train_dataset = SMILESDataset(tokenizer, train_data, train_targets, augment=True, max_length=max_length)\n",
        "val_dataset = SMILESDataset(tokenizer, val_data, val_targets, augment=False, max_length=max_length)\n",
        "test_dataset = SMILESDataset(tokenizer, test_data, test_targets, augment=False, max_length=max_length)\n",
        "\n",
        "# batch size could potentially be increased with lots of data\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transformer block\n",
        "# https://pubs.rsc.org/en/content/articlelanding/2022/dd/d2dd00058j\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(ff_dim, embed_dim)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Multi-head attention with residual connection\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        x = self.layer_norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        # Feed-forward network with residual connection\n",
        "        mlp_output = self.mlp(x)\n",
        "        x = self.layer_norm2(x + self.dropout(mlp_output))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / embed_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_layers, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.positional_encoding = PositionalEncoding(embed_dim, max_len)\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
        "        self.regression_head = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(embed_dim // 2, 1)  # Output a single value\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input embedding + positional encoding\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "        # Pass through Transformer blocks\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Apply LayerNorm and regression head\n",
        "        x = self.layer_norm(x)\n",
        "        x = x.mean(dim=1)  # Global pooling (mean over sequence length); this type of pooling should probably be changed\n",
        "        x = self.regression_head(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 1])\n"
          ]
        }
      ],
      "source": [
        "# Inital guess for parameters\n",
        "vocab_size = len(tokenizer.vocab)  # Vocabulary size from tokenizer\n",
        "embed_dim = 128  # Embedding dimension\n",
        "num_heads = 4  # Number of attention heads; research further the relation between embed dim and heads\n",
        "ff_dim =  512 # Feed-forward network dimension; typically 4 times embedding so 1024\n",
        "num_layers = 6  # Number of Transformer layers\n",
        "max_len = 100  # Maximum sequence length; should be checked based on tokenization\n",
        "\n",
        "model = TransformerDecoder(vocab_size, embed_dim, num_heads, ff_dim, num_layers, max_len)\n",
        "\n",
        "# Example input (batch of tokenized SMILES)\n",
        "input_tokens = torch.randint(0, vocab_size, (32, max_len))  # Batch of 32 sequences\n",
        "output = model(input_tokens)  # Output shape: (32, 1)\n",
        "print(output.shape)  # Should be [32, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code to train the model \n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=1e-3):\n",
        "    model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for i, (batch, targets) in enumerate(train_loader):\n",
        "            batch = batch.to(device)\n",
        "            targets = targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch)\n",
        "            loss = criterion(outputs.squeeze(), targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        train_losses.append(avg_loss)\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Validation step\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for i, (batch, targets) in enumerate(val_loader):\n",
        "                batch = batch.to(device)\n",
        "                targets = targets.to(device)\n",
        "                outputs = model(batch)\n",
        "                loss = criterion(outputs.squeeze(), targets)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss', marker='o')\n",
        "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss', marker='o')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss Curves')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Training Loss: 0.0500\n",
            "Validation Loss: 0.0137\n",
            "Epoch 2/10, Training Loss: 0.0086\n",
            "Validation Loss: 0.0072\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model;learning rate potentially lower is better \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer_decoder.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[1;32mIn[17], line 17\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, learning_rate)\u001b[0m\n\u001b[0;32m     15\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(), targets)\n\u001b[0;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[1;32mc:\\Users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[15], line 23\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Pass through Transformer blocks\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks:\n\u001b[1;32m---> 23\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Apply LayerNorm and regression head\u001b[39;00m\n\u001b[0;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(x)\n",
            "File \u001b[1;32mc:\\Users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[13], line 19\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Multi-head attention with residual connection\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     attn_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(x, x, x)\n\u001b[1;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Feed-forward network with residual connection\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     mlp_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(x)\n",
            "File \u001b[1;32mc:\\Users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\veraj\\anaconda3\\envs\\mlcourse\\lib\\site-packages\\torch\\nn\\functional.py:1425\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 1425\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1426\u001b[0m )\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Train the model;learning rate potentially lower is better \n",
        "train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=1e-4)\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'transformer_decoder.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Evaluate the model on the test set\n",
        "# def evaluate_model(model, test_loader):\n",
        "#     model.to(device)\n",
        "#     model.eval()\n",
        "#     total_loss = 0.0\n",
        "#     criterion = nn.MSELoss()\n",
        "#     all_targets = []\n",
        "#     all_predictions = []\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for batch, targets in test_loader:\n",
        "#             batch = batch.to(device)\n",
        "#             targets = targets.to(device)\n",
        "#             outputs = model(batch)\n",
        "#             loss = criterion(outputs.squeeze(), targets)\n",
        "#             total_loss += loss.item()\n",
        "\n",
        "#             # Collect predictions and targets for MAE and R²\n",
        "#             all_predictions.extend(outputs.squeeze().cpu().numpy())\n",
        "#             all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "#     avg_loss = total_loss / len(test_loader)\n",
        "#     mae = mean_absolute_error(all_targets, all_predictions)\n",
        "#     r2 = r2_score(all_targets, all_predictions)\n",
        "#     apd = np.mean([\n",
        "#         np.abs((pred - target) / target) for pred, target in zip(all_predictions, all_targets) if target != 0\n",
        "#     ]) # Average Percentage Deviation based on \"Understanding the language of molecules...\" paper\n",
        "\n",
        "#     print(f\"Test Loss (MSE): {avg_loss:.4f}\")\n",
        "#     print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "#     print(f\"R² Score: {r2:.4f}\")\n",
        "#     print(f\"Average Percentage Deviation (APD): {apd:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "test_losses = []\n",
        "test_mae = []\n",
        "\n",
        "# Ensure std and mu are tensors and on the correct device\n",
        "if not isinstance(std, torch.Tensor):\n",
        "    std = torch.tensor(std)\n",
        "if not isinstance(mu, torch.Tensor):\n",
        "    mu = torch.tensor(mu)\n",
        "std = std.to(device)\n",
        "mu = mu.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch, targets in test_loader:\n",
        "        batch = batch.to(device)\n",
        "        targets = targets.to(device)\n",
        "        \n",
        "        preds = model(batch)\n",
        "        original_preds = preds * std + mu\n",
        "        original_targets = targets * std + mu\n",
        "\n",
        "        mse_loss = F.mse_loss(original_preds, original_targets)\n",
        "        mae_loss = F.l1_loss(original_preds, original_targets)\n",
        "        \n",
        "        test_losses.append(mse_loss.item())\n",
        "        test_mae.append(mae_loss.item())\n",
        "\n",
        "mean_test_loss = sum(test_losses) / len(test_losses)\n",
        "mean_test_mae = sum(test_mae) / len(test_mae)\n",
        "\n",
        "print(f\"Test MSE loss (original scale): {mean_test_loss:.4f}\")\n",
        "print(f\"Test MAE loss (original scale): {mean_test_mae:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0zvKnpLGf9v"
      },
      "source": [
        "## Task 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AC1KICrZGgkY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mIuOY4BGxqU"
      },
      "source": [
        "## Task 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_NgxsO3GxEE"
      },
      "outputs": [],
      "source": [
        "def is_valid_smiles(smiles):\n",
        "    if smiles is None:\n",
        "        return False\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        return mol is not None\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def canonicalize(smiles):\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol:\n",
        "            return Chem.MolToSmiles(mol, canonical=True)\n",
        "        return 'None'\n",
        "    except:\n",
        "        return 'None'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN_jGgOwG4kK"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('COO', 'COO')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "canonicalize(\"COO\"), canonicalize(\"O(C)O\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bhjYhYrHCuQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(True, True, False)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "is_valid_smiles(\"COO\"), is_valid_smiles(\"O(C)O\"), is_valid_smiles(\"C##\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_YgzDpMH-Vl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mlcourse",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
