{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wv9q_pcGshE"
      },
      "source": [
        "# Assignment 2 2AMM10 2023-2024\n",
        "\n",
        "## Group: [Fill in your group name]\n",
        "### Member 1: [Fill in your name]\n",
        "### Member 2: [Fill in your name]\n",
        "### Member 3: [Fill in your name]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQzvuDWw_Eyw"
      },
      "source": [
        "We need to install some specific libraries. The cell below installs torch_geometric for torch 2.6.0+cu124. In case the current version of torch is different, check [here](https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html) to see which versions (of both libraries) you should install. You might also need to install an old version of torch from [here](https://pytorch.org/get-started/previous-versions/)\n",
        "\n",
        "**Note:** Do not install pyg_lib from the optional dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ibC2lMHfD67H"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: torch\n",
            "Version: 2.6.0\n",
            "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
            "Home-page: https://pytorch.org/\n",
            "Author: PyTorch Team\n",
            "Author-email: packages@pytorch.org\n",
            "License: BSD-3-Clause\n",
            "Location: C:\\Users\\20182672\\AppData\\Local\\anaconda3\\envs\\MLEngineering\\Lib\\site-packages\n",
            "Requires: filelock, fsspec, jinja2, networkx, setuptools, sympy, typing-extensions\n",
            "Required-by: accelerate, flair, openml-pytorch, pytorch-lightning, pytorch_revgrad, torchmetrics, torchvision, transformer-smaller-training-vocab\n"
          ]
        }
      ],
      "source": [
        "!pip show torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8qrPQFNe_AJu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rdkit in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (2025.3.2)\n",
            "Requirement already satisfied: numpy in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (from rdkit) (10.4.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch_geometric in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (from torch_geometric) (3.11.14)\n",
            "Requirement already satisfied: fsspec in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (from torch_geometric) (7.0.0)\n",
            "Requirement already satisfied: pyparsing in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (from aiohttp->torch_geometric) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (from aiohttp->torch_geometric) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (from aiohttp->torch_geometric) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (from requests->torch_geometric) (2025.4.26)\n",
            "Requirement already satisfied: colorama in c:\\users\\20182672\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->torch_geometric) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Requirement already satisfied: torch_scatter in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (2.1.2+pt26cu124)\n",
            "Requirement already satisfied: torch_sparse in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (0.6.18+pt26cu124)\n",
            "Requirement already satisfied: torch_cluster in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (1.6.3+pt26cu124)\n",
            "Requirement already satisfied: torch_spline_conv in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (1.2.2+pt26cu124)\n",
            "Requirement already satisfied: scipy in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (from torch_sparse) (1.15.2)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\20182672\\appdata\\local\\anaconda3\\envs\\mlengineering\\lib\\site-packages (from scipy->torch_sparse) (1.26.4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit\n",
        "!pip install torch_geometric\n",
        "!pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.6.0+cu124.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WVL2eo0g_Iuv"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw, AllChem\n",
        "from rdkit import RDLogger\n",
        "RDLogger.DisableLog('rdApp.*')\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from rdkit import Chem\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "H8rvaK56_iQ7"
      },
      "outputs": [],
      "source": [
        "with open('pos_data.pkl', 'rb') as f:\n",
        "    pos_data = pickle.load(f)\n",
        "\n",
        "with open('type_data.pkl', 'rb') as f:\n",
        "    type_data = pickle.load(f)\n",
        "\n",
        "with open('smiles.pkl', 'rb') as f:\n",
        "    smiles_data = pickle.load(f)\n",
        "\n",
        "data_split = np.load('data_split.npz')\n",
        "\n",
        "train_idxes = data_split['train_idx']\n",
        "test_idxes = data_split['test_idx']\n",
        "\n",
        "formation_energy = np.load('formation_energy.npz')\n",
        "\n",
        "fe = formation_energy['y'] # normalized formation energy\n",
        "mu = formation_energy['mu']\n",
        "std = formation_energy['sigma']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DIsGRQcxA_4Q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of data\n",
            "pos_data: 129012, type_data: 129012, smiles: 129012\n",
            "Idxes\n",
            "train: 119012, test: 10000, sum: 129012\n"
          ]
        }
      ],
      "source": [
        "# shapes of lists\n",
        "print(\"Length of data\")\n",
        "print(f\"pos_data: {len(pos_data)}, type_data: {len(type_data)}, smiles: {len(smiles_data)}\")\n",
        "print(\"Idxes\")\n",
        "print(f\"train: {len(train_idxes)}, test: {len(test_idxes)}, sum: {len(train_idxes) + len(test_idxes)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bVDJF7I3BFa2"
      },
      "outputs": [],
      "source": [
        "def at_number_to_atom_name(at_number):\n",
        "    if at_number == 6:\n",
        "        return 'C'\n",
        "    elif at_number == 1:\n",
        "        return 'H'\n",
        "    elif at_number == 7:\n",
        "        return 'N'\n",
        "    elif at_number == 8:\n",
        "        return 'O'\n",
        "    elif at_number == 9:\n",
        "        return 'F'\n",
        "    elif at_number == 16:\n",
        "        return 'S'\n",
        "    else:\n",
        "        return 'Unknown'\n",
        "\n",
        "def inspect_structure(idx):\n",
        "    smile = smiles_data[idx]\n",
        "    pos = pos_data[idx]\n",
        "    typ = type_data[idx]\n",
        "\n",
        "    header = f\"{'Atom':^5}│{'Number':^6}│{'x':^10}│{'y':^10}│{'z':^10}\"\n",
        "    line   = \"─────┼──────┼──────────┼──────────┼──────────\"\n",
        "    print(header)\n",
        "    print(line)\n",
        "\n",
        "    for atom_num, (x, y, z) in zip(typ, pos):\n",
        "        atom_sym = at_number_to_atom_name(atom_num)\n",
        "        print(f\"{atom_sym:^5}│{atom_num:^6}│{x:>10.3f}│{y:>10.3f}│{z:>10.3f}\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(f'SMILE: {smile}')\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(f'Formation Energy: {fe[idx]*std + mu:.3f}')\n",
        "    print(f'Formation Energy (normalized): {fe[idx]:.5f}')\n",
        "    mol = Chem.MolFromSmiles(smile)\n",
        "    if mol:\n",
        "        # RDKit prefers 2‑D coordinates for nice depictions\n",
        "        Chem.AllChem.Compute2DCoords(mol)\n",
        "        img = Draw.MolToImage(mol, size=(300, 300))\n",
        "\n",
        "        # Display with matplotlib (works both in notebooks and scripts)\n",
        "        plt.figure(figsize=(3, 3))\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K1rs7hhCC4oq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Atom │Number│    x     │    y     │    z     \n",
            "─────┼──────┼──────────┼──────────┼──────────\n",
            "  C  │  6   │    -0.013│     1.086│     0.008\n",
            "  H  │  1   │     0.002│    -0.006│     0.002\n",
            "  H  │  1   │     1.012│     1.464│     0.000\n",
            "  H  │  1   │    -0.541│     1.447│    -0.877\n",
            "  H  │  1   │    -0.524│     1.438│     0.906\n",
            "\n",
            "\n",
            "SMILE: C\n",
            "\n",
            "\n",
            "Formation Energy: -17.172\n",
            "Formation Energy (normalized): 5.72327\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# methane\n",
        "# Note how methane has a relatively high formation energy (compared to QM9)\n",
        "# This correlates with lower thermodynamic stability and higher reactivity\n",
        "# For example, methane readily burns in oxygen (CH₄ + 2O₂ → CO₂ + 2H₂O)\n",
        "inspect_structure(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vo8hYLuQCeBR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Atom │Number│    x     │    y     │    z     \n",
            "─────┼──────┼──────────┼──────────┼──────────\n",
            "  C  │  6   │    -0.143│     1.521│     0.693\n",
            "  C  │  6   │     0.003│     0.009│     0.543\n",
            "  C  │  6   │     0.619│    -0.462│    -0.800\n",
            "  C  │  6   │     1.284│    -1.791│    -0.451\n",
            "  C  │  6   │     2.629│    -1.823│     0.182\n",
            "  C  │  6   │     3.588│    -0.659│     0.233\n",
            "  O  │  8   │     4.471│    -0.752│     1.344\n",
            "  N  │  7   │     1.408│    -1.923│     1.022\n",
            "  C  │  6   │     0.902│    -0.661│     1.609\n",
            "  H  │  1   │     0.830│     2.022│     0.609\n",
            "  H  │  1   │    -0.798│     1.937│    -0.079\n",
            "  H  │  1   │    -0.570│     1.786│     1.666\n",
            "  H  │  1   │    -0.991│    -0.449│     0.625\n",
            "  H  │  1   │    -0.142│    -0.594│    -1.573\n",
            "  H  │  1   │     1.332│     0.276│    -1.184\n",
            "  H  │  1   │     0.937│    -2.679│    -0.974\n",
            "  H  │  1   │     3.162│    -2.773│     0.153\n",
            "  H  │  1   │     4.226│    -0.685│    -0.658\n",
            "  H  │  1   │     3.069│     0.309│     0.222\n",
            "  H  │  1   │     3.936│    -0.919│     2.127\n",
            "  H  │  1   │     1.713│     0.013│     1.915\n",
            "  H  │  1   │     0.338│    -0.908│     2.513\n",
            "\n",
            "\n",
            "SMILE: C[C@H]1C[C@H]2[C@@H](CO)[N@H+]2C1\n",
            "\n",
            "\n",
            "Formation Energy: -88.273\n",
            "Formation Energy (normalized): -1.17243\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# random structure\n",
        "inspect_structure(np.random.choice(range(len(smiles_data))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The available device is cpu\n"
          ]
        }
      ],
      "source": [
        "# Check device settings\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    device = 'cpu'\n",
        "\n",
        "print(f'The available device is {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of SMILES data: 129012\n",
            "First 10 SMILES strings:\n",
            "0: C\n",
            "1: N\n",
            "2: O\n",
            "3: C#C\n",
            "4: C#N\n",
            "5: C=O\n",
            "6: CC\n",
            "7: CO\n",
            "8: C#CC\n",
            "9: CC#N\n",
            "Max length of SMILES strings: 62\n"
          ]
        }
      ],
      "source": [
        "# These are some print statements inserted by me to check the data\n",
        "\n",
        "# Check the length of the SMILES representation\n",
        "print(f'Length of SMILES data: {len(smiles_data)}')\n",
        "# Check the first 10 SMILES strings\n",
        "print(\"First 10 SMILES strings:\")\n",
        "for i in range(10):\n",
        "    print(f\"{i}: {smiles_data[i]}\")\n",
        "# Check max lenght of SMILES strings\n",
        "max_smiles_length = max(len(smile) for smile in smiles_data) # suggestion for max lenght in model\n",
        "print(f'Max length of SMILES strings: {max_smiles_length}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76MeHNQ_Gd9t"
      },
      "source": [
        "## Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PART II: SMILES\n",
        "- String sequence to continous values\n",
        "- Permutational invariance should be taken into account with canonical SMILES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Lv736iffCez4"
      },
      "outputs": [],
      "source": [
        "# Tokenize the SMILES strings\n",
        "# https://jcheminf.biomedcentral.com/articles/10.1186/s13321-023-00725-9\n",
        "# Unclear to me whether we want to encode to values, but seems logical\n",
        "class SMILESAISTokenizer:\n",
        "    def __init__(self):\n",
        "        self.vocab = set()\n",
        "        self.special_tokens = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
        "        self.token_to_idx = {}\n",
        "        self.idx_to_token = {}\n",
        "    \n",
        "    def build_vocabulary(self, smiles_list):\n",
        "        tokens = set()\n",
        "\n",
        "        for smi in smiles_list:\n",
        "            tokens.update(self.tokenize(smi))\n",
        "\n",
        "        tokens = sorted(list(tokens))\n",
        "        full_vocab = self.special_tokens + tokens\n",
        "        self.vocab = full_vocab\n",
        "        self.token_to_idx = {tok: i for i, tok in enumerate(full_vocab)}\n",
        "        self.idx_to_token = {i: tok for tok, i in self.token_to_idx.items()}\n",
        "\n",
        "        return full_vocab\n",
        "    \n",
        "    def tokenize(self, smiles):\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            return []\n",
        "\n",
        "        tokens = []\n",
        "        for atom in mol.GetAtoms():\n",
        "            symbol = atom.GetSymbol()\n",
        "            if atom.GetIsAromatic():\n",
        "                symbol = symbol.lower()\n",
        "\n",
        "            charge = atom.GetFormalCharge()\n",
        "            chiral = str(atom.GetChiralTag())\n",
        "            hs = atom.GetTotalNumHs()\n",
        "            ring = 'R' if atom.IsInRing() else '!R'\n",
        "            neighbors = sorted([nbr.GetSymbol() for nbr in atom.GetNeighbors()])\n",
        "\n",
        "            token = f\"{symbol}:{charge}:{chiral}:{hs}:{ring}:[{','.join(neighbors)}]\"\n",
        "            tokens.append(token)\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def encode(self, smiles, max_length=None):\n",
        "        tokens = self.tokenize(smiles)\n",
        "        # Add <SOS> and <EOS> tokens to learn start and end of sequence\n",
        "        tokens = ['<SOS>'] + tokens + ['<EOS>']\n",
        "        encoded = [self.token_to_idx.get(tok, self.token_to_idx['<UNK>']) for tok in tokens]\n",
        "\n",
        "        if max_length:\n",
        "            encoded = encoded[:max_length]\n",
        "            encoded += [self.token_to_idx['<PAD>']] * (max_length - len(encoded))\n",
        "        return encoded\n",
        "\n",
        "    def decode(self, indices):\n",
        "        tokens = [self.idx_to_token.get(idx, '<UNK>') for idx in indices]\n",
        "        tokens = [tok for tok in tokens if tok not in self.special_tokens]\n",
        "        return tokens # Not sure whether we need decoder\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # TOY TEST\n",
        "# smiles_list = [\n",
        "#     \"CCO\",         # ethanol\n",
        "#     \"c1ccccc1\",    # benzene\n",
        "#     \"C[N+](C)(C)C\",# tetramethylammonium\n",
        "#     \"O=C=O\"        # carbon dioxide\n",
        "# ]\n",
        "\n",
        "# tokenizer = SMILESAISTokenizer()\n",
        "# # Initialize and build vocab\n",
        "# vocab = tokenizer.build_vocabulary(smiles_list)\n",
        "# print(\"Vocabulary size:\", len(vocab))\n",
        "\n",
        "# # Tokenize and encode a molecule\n",
        "# encoded = tokenizer.encode(\"CCO\", max_length=10)\n",
        "# print(\"Encoded:\", encoded)\n",
        "# print(\"Decoded tokens:\", tokenizer.decode(encoded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SMILESDataset(Dataset):\n",
        "    def __init__(self, tokenizer, smiles_list, formation_energy, augment=True, max_length=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.smiles = smiles_list\n",
        "        self.augment = augment\n",
        "        self.max_length = max_length\n",
        "        self.formation_energy = formation_energy\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.smiles)\n",
        "\n",
        "    def randomize_smiles(self, mol):\n",
        "        return Chem.MolToSmiles(mol, doRandom=True)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        smiles = self.smiles[idx]\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            return self.__getitem__((idx + 1) % len(self.smiles))  # fallback\n",
        "\n",
        "        if self.augment:\n",
        "            smiles = self.randomize_smiles(mol)\n",
        "        else:\n",
        "            smiles = Chem.MolToSmiles(mol, canonical=True)\n",
        "\n",
        "        encoded = self.tokenizer.encode(smiles, max_length=self.max_length)\n",
        "        target = self.formation_energy[idx]\n",
        "        return torch.tensor(encoded, dtype=torch.long), torch.tensor(target, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create tokenzier and dataset\n",
        "tokenizer = SMILESAISTokenizer()\n",
        "tokenizer.build_vocabulary(smiles_data)\n",
        "max_length = 100 # the  maximum length of SMILES strings is 62, tokenization might change that\n",
        "\n",
        "# Split data into train and validation 80/20, already having a test dataset\n",
        "train_data = [smiles_data[i] for i in train_idxes[:-int(0.2 * len(train_idxes))]] # select first 80% of train_idxes; no overlap measured\n",
        "val_data = [smiles_data[i] for i in train_idxes[-int(0.2 * len(train_idxes)):]] # select last 20% of train_idxes; no overlap measured\n",
        "test_data = [smiles_data[i] for i in test_idxes]\n",
        "\n",
        "train_targets = fe[train_idxes[:-int(0.2 * len(train_idxes))]]\n",
        "val_targets = fe[train_idxes[-int(0.2 * len(train_idxes)):]]\n",
        "test_targets = fe[test_idxes]\n",
        "\n",
        "train_dataset = SMILESDataset(tokenizer, train_data, train_targets, augment=True, max_length=max_length)\n",
        "val_dataset = SMILESDataset(tokenizer, val_data, val_targets, augment=False, max_length=max_length)\n",
        "test_dataset = SMILESDataset(tokenizer, test_data, test_targets, augment=False, max_length=max_length)\n",
        "\n",
        "# batch size could potentially be increased with lots of data\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transformer block\n",
        "# https://pubs.rsc.org/en/content/articlelanding/2022/dd/d2dd00058j\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(ff_dim, embed_dim)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Multi-head attention with residual connection\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        x = self.layer_norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        # Feed-forward network with residual connection\n",
        "        mlp_output = self.mlp(x)\n",
        "        x = self.layer_norm2(x + self.dropout(mlp_output))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / embed_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_layers, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.positional_encoding = PositionalEncoding(embed_dim, max_len)\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
        "        self.regression_head = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(embed_dim // 2, 1)  # Output a single value\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input embedding + positional encoding\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "        # Pass through Transformer blocks\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Apply LayerNorm and regression head\n",
        "        x = self.layer_norm(x)\n",
        "        x = x.mean(dim=1)  # Global pooling (mean over sequence length); this type of pooling should probably be changed\n",
        "        x = self.regression_head(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 1])\n"
          ]
        }
      ],
      "source": [
        "# Inital guess for parameters\n",
        "vocab_size = len(tokenizer.vocab)  # Vocabulary size from tokenizer\n",
        "embed_dim = 128  # Embedding dimension\n",
        "num_heads = 4  # Number of attention heads; research further the relation between embed dim and heads\n",
        "ff_dim =  512 # Feed-forward network dimension; typically 4 times embedding so 1024\n",
        "num_layers = 6  # Number of Transformer layers\n",
        "max_len = 100  # Maximum sequence length; should be checked based on tokenization\n",
        "\n",
        "model = TransformerDecoder(vocab_size, embed_dim, num_heads, ff_dim, num_layers, max_len)\n",
        "\n",
        "# Example input (batch of tokenized SMILES)\n",
        "input_tokens = torch.randint(0, vocab_size, (32, max_len))  # Batch of 32 sequences\n",
        "output = model(input_tokens)  # Output shape: (32, 1)\n",
        "print(output.shape)  # Should be [32, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code to train the model \n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=1e-3):\n",
        "    model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for i, (batch, targets) in enumerate(train_loader):\n",
        "            batch = batch.to(device)\n",
        "            targets = targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch)\n",
        "            loss = criterion(outputs.squeeze(), targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        train_losses.append(avg_loss)\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Validation step\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for i, (batch, targets) in enumerate(val_loader):\n",
        "                batch = batch.to(device)\n",
        "                targets = targets.to(device)\n",
        "                outputs = model(batch)\n",
        "                loss = criterion(outputs.squeeze(), targets)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss', marker='o')\n",
        "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss', marker='o')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss Curves')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Training Loss: 0.9989\n",
            "Validation Loss: 1.0294\n",
            "Epoch 2/10, Training Loss: 0.9978\n",
            "Validation Loss: 1.0295\n"
          ]
        }
      ],
      "source": [
        "# Train the model;learning rate potentially lower is better \n",
        "train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=1e-3)\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'transformer_decoder.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0zvKnpLGf9v"
      },
      "source": [
        "## Task 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AC1KICrZGgkY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mIuOY4BGxqU"
      },
      "source": [
        "## Task 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_NgxsO3GxEE"
      },
      "outputs": [],
      "source": [
        "def is_valid_smiles(smiles):\n",
        "    if smiles is None:\n",
        "        return False\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        return mol is not None\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def canonicalize(smiles):\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol:\n",
        "            return Chem.MolToSmiles(mol, canonical=True)\n",
        "        return 'None'\n",
        "    except:\n",
        "        return 'None'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN_jGgOwG4kK"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('COO', 'COO')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "canonicalize(\"COO\"), canonicalize(\"O(C)O\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bhjYhYrHCuQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(True, True, False)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "is_valid_smiles(\"COO\"), is_valid_smiles(\"O(C)O\"), is_valid_smiles(\"C##\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_YgzDpMH-Vl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "MLEngineering",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
